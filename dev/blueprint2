# vllm_inference_server.py
"""
vLLM 기반 고성능 추론 서버
간단한 설정과 관리를 위한 래퍼
"""

import asyncio
import logging
import time
import json
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any
from contextlib import asynccontextmanager

import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from vllm import LLM, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
import torch
import psutil
import GPUtil

# 로깅 설정
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# 데이터 모델
# ============================================================================

class InferenceRequest(BaseModel):
    """추론 요청 모델"""
    agent_type: str = Field(..., description="에이전트 타입")
    prompt: str = Field(..., description="입력 프롬프트")
    context: Dict[str, Any] = Field(default={}, description="컨텍스트")
    max_tokens: int = Field(default=2048, description="최대 토큰 수")
    temperature: float = Field(default=0.1, description="생성 온도")
    top_p: float = Field(default=0.9, description="Top-p")
    top_k: int = Field(default=50, description="Top-k")
    stop: Optional[List[str]] = Field(default=None, description="중지 토큰")

class InferenceResponse(BaseModel):
    """추론 응답 모델"""
    request_id: str
    agent_type: str
    response: str
    tokens_generated: int
    response_time: float
    timestamp: datetime
    model_name: str

class BatchRequest(BaseModel):
    """배치 요청 모델"""
    requests: List[InferenceRequest]
    batch_size: int = Field(default=10, description="배치 크기")

class SystemStatus(BaseModel):
    """시스템 상태 모델"""
    status: str
    model_loaded: bool
    gpu_count: int
    memory_usage: Dict[str, float]
    active_requests: int
    total_requests: int
    uptime: float
    throughput: float  # tokens/sec

# ============================================================================
# vLLM 엔진 매니저
# ============================================================================

class VLLMManager:
    """vLLM 엔진 관리 클래스"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-large", 
                 tensor_parallel_size: int = 1,
                 max_model_len: int = 4096,
                 gpu_memory_utilization: float = 0.9):
        
        self.model_name = model_name
        self.tensor_parallel_size = tensor_parallel_size
        self.max_model_len = max_model_len
        self.gpu_memory_utilization = gpu_memory_utilization
        
        self.engine = None
        self.is_loaded = False
        self.request_count = 0
        self.active_requests = 0
        self.total_tokens_generated = 0
        self.start_time = time.time()
        
        # 에이전트별 시스템 프롬프트
        self.system_prompts = {
            "orchestration": """당신은 제조업 현장의 AI 오케스트레이션 시스템입니다.
사용자의 자연어 질의를 분석하여 적절한 에이전트 조합과 실행 계획을 수립하세요.
응답은 JSON 형식으로 구조화하여 제공하세요.""",

            "monitoring": """당신은 제조 공정 모니터링 전문가입니다.
센서 데이터와 이상 탐지 결과를 분석하여 현장 작업자가 이해하기 쉬운 
명확한 설명, 원인 분석, 위험도 평가, 권장 조치를 제공하세요.""",

            "prediction": """당신은 제조 공정 예측 분석 전문가입니다.
예측 모델의 결과를 해석하여 신뢰도, 불확실성, 위험 시나리오, 
권장 제어 액션을 현장에서 이해하기 쉬운 언어로 설명하세요.""",

            "control": """당신은 제조 공정 자율제어 전문가입니다.
제어 의사결정의 근거, 예상 효과, 잠재적 부작용, 대안을 설명하고
안전성을 최우선으로 고려한 제어 액션을 추천하세요."""
        }
    
    async def initialize(self):
        """vLLM 엔진 초기화"""
        try:
            logger.info(f"Initializing vLLM engine with model: {self.model_name}")
            
            # 엔진 설정
            engine_args = AsyncEngineArgs(
                model=self.model_name,
                tensor_parallel_size=self.tensor_parallel_size,
                max_model_len=self.max_model_len,
                gpu_memory_utilization=self.gpu_memory_utilization,
                disable_log_stats=True,
                enforce_eager=False,  # CUDA 그래프 사용으로 성능 향상
                max_num_seqs=256,     # 높은 동시성 지원
                max_num_batched_tokens=8192,  # 배치 처리 최적화
            )
            
            # 비동기 엔진 생성
            self.engine = AsyncLLMEngine.from_engine_args(engine_args)
            self.is_loaded = True
            
            logger.info("vLLM engine initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize vLLM engine: {e}")
            raise
    
    async def generate(self, request: InferenceRequest) -> InferenceResponse:
        """텍스트 생성"""
        if not self.is_loaded:
            raise HTTPException(status_code=500, detail="Model not loaded")
        
        start_time = time.time()
        request_id = str(uuid.uuid4())
        
        try:
            self.active_requests += 1
            self.request_count += 1
            
            # 시스템 프롬프트와 사용자 프롬프트 결합
            system_prompt = self.system_prompts.get(request.agent_type, "")
            
            if request.context:
                context_str = "\n".join([f"{k}: {v}" for k, v in request.context.items()])
                full_prompt = f"{system_prompt}\n\n컨텍스트:\n{context_str}\n\n사용자: {request.prompt}\n\n응답:"
            else:
                full_prompt = f"{system_prompt}\n\n사용자: {request.prompt}\n\n응답:"
            
            # 샘플링 파라미터 설정
            sampling_params = SamplingParams(
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                max_tokens=request.max_tokens,
                stop=request.stop or ["사용자:", "\n\n사용자:"],
                skip_special_tokens=True,
            )
            
            # 생성 요청
            results = []
            async for output in self.engine.generate(
                full_prompt, 
                sampling_params, 
                request_id
            ):
                results.append(output)
            
            # 마지막 결과 추출
            final_output = results[-1]
            generated_text = final_output.outputs[0].text.strip()
            tokens_generated = len(final_output.outputs[0].token_ids)
            
            # 통계 업데이트
            self.total_tokens_generated += tokens_generated
            response_time = time.time() - start_time
            
            return InferenceResponse(
                request_id=request_id,
                agent_type=request.agent_type,
                response=generated_text,
                tokens_generated=tokens_generated,
                response_time=response_time,
                timestamp=datetime.now(),
                model_name=self.model_name
            )
            
        except Exception as e:
            logger.error(f"Generation error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
        
        finally:
            self.active_requests -= 1
    
    async def batch_generate(self, batch_request: BatchRequest) -> List[InferenceResponse]:
        """배치 생성"""
        if len(batch_request.requests) > batch_request.batch_size:
            raise HTTPException(
                status_code=400, 
                detail=f"Batch size too large (max {batch_request.batch_size})"
            )
        
        # 병렬로 모든 요청 처리
        tasks = [self.generate(req) for req in batch_request.requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 성공한 결과만 반환
        successful_results = []
        for result in results:
            if isinstance(result, InferenceResponse):
                successful_results.append(result)
            else:
                logger.error(f"Batch generation error: {result}")
        
        return successful_results
    
    def get_system_status(self) -> SystemStatus:
        """시스템 상태 조회"""
        memory_info = psutil.virtual_memory()
        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        # GPU 메모리 정보
        gpu_memory = {}
        if gpu_count > 0:
            try:
                gpus = GPUtil.getGPUs()
                for i, gpu in enumerate(gpus):
                    gpu_memory[f"gpu_{i}_memory_used"] = gpu.memoryUsed
                    gpu_memory[f"gpu_{i}_memory_total"] = gpu.memoryTotal
                    gpu_memory[f"gpu_{i}_utilization"] = gpu.load * 100
            except:
                pass
        
        # 처리량 계산 (tokens/sec)
        uptime = time.time() - self.start_time
        throughput = self.total_tokens_generated / uptime if uptime > 0 else 0
        
        return SystemStatus(
            status="healthy" if self.is_loaded else "loading",
            model_loaded=self.is_loaded,
            gpu_count=gpu_count,
            memory_usage={
                "ram_used_gb": memory_info.used / (1024**3),
                "ram_total_gb": memory_info.total / (1024**3),
                "ram_percent": memory_info.percent,
                **gpu_memory
            },
            active_requests=self.active_requests,
            total_requests=self.request_count,
            uptime=uptime,
            throughput=throughput
        )

# ============================================================================
# FastAPI 애플리케이션
# ============================================================================

# 전역 변수
vllm_manager = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """애플리케이션 생명주기 관리"""
    global vllm_manager
    
    # 시작 시 vLLM 엔진 초기화
    vllm_manager = VLLMManager(
        model_name="microsoft/DialoGPT-large",  # 필요에 따라 변경
        tensor_parallel_size=torch.cuda.device_count() if torch.cuda.is_available() else 1,
        max_model_len=4096,
        gpu_memory_utilization=0.9
    )
    
    await vllm_manager.initialize()
    logger.info("vLLM Inference Server started successfully")
    
    yield
    
    # 종료 시 정리
    logger.info("Shutting down vLLM Inference Server")

# FastAPI 앱 생성
app = FastAPI(
    title="vLLM High-Performance Inference Server",
    description="제조업 Multi-Agent 시스템용 고성능 vLLM 추론 서버",
    version="1.0.0",
    lifespan=lifespan
)

# CORS 설정
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# API 엔드포인트
# ============================================================================

@app.get("/")
async def root():
    """루트 엔드포인트"""
    return {
        "message": "vLLM High-Performance Inference Server", 
        "status": "running",
        "model": vllm_manager.model_name if vllm_manager else "loading"
    }

@app.get("/health", response_model=SystemStatus)
async def health_check():
    """헬스 체크"""
    if not vllm_manager:
        raise HTTPException(status_code=503, detail="Server not ready")
    
    return vllm_manager.get_system_status()

@app.post("/api/v1/generate", response_model=InferenceResponse)
async def generate_text(
    request: InferenceRequest,
    background_tasks: BackgroundTasks
):
    """단일 텍스트 생성"""
    try:
        response = await vllm_manager.generate(request)
        
        # 백그라운드 로깅
        background_tasks.add_task(log_request, request, response)
        
        return response
        
    except Exception as e:
        logger.error(f"Generation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/batch_generate")
async def batch_generate_text(batch_request: BatchRequest):
    """배치 텍스트 생성"""
    try:
        responses = await vllm_manager.batch_generate(batch_request)
        
        return {
            "batch_id": str(uuid.uuid4()),
            "total_requests": len(batch_request.requests),
            "successful": len(responses),
            "failed": len(batch_request.requests) - len(responses),
            "results": responses
        }
        
    except Exception as e:
        logger.error(f"Batch generation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/models")
async def get_model_info():
    """모델 정보"""
    if not vllm_manager:
        raise HTTPException(status_code=503, detail="Server not ready")
    
    return {
        "current_model": vllm_manager.model_name,
        "tensor_parallel_size": vllm_manager.tensor_parallel_size,
        "max_model_len": vllm_manager.max_model_len,
        "supported_agents": list(vllm_manager.system_prompts.keys()),
        "gpu_count": torch.cuda.device_count(),
        "is_loaded": vllm_manager.is_loaded
    }

@app.get("/api/v1/stats")
async def get_statistics():
    """성능 통계"""
    if not vllm_manager:
        raise HTTPException(status_code=503, detail="Server not ready")
    
    status = vllm_manager.get_system_status()
    
    return {
        "requests": {
            "total": vllm_manager.request_count,
            "active": vllm_manager.active_requests,
            "completed": vllm_manager.request_count - vllm_manager.active_requests
        },
        "performance": {
            "throughput_tokens_per_sec": status.throughput,
            "total_tokens_generated": vllm_manager.total_tokens_generated,
            "uptime_seconds": status.uptime,
            "average_tokens_per_request": (
                vllm_manager.total_tokens_generated / vllm_manager.request_count 
                if vllm_manager.request_count > 0 else 0
            )
        },
        "system": {
            "gpu_count": status.gpu_count,
            "memory_usage": status.memory_usage
        }
    }

# ============================================================================
# 유틸리티 함수
# ============================================================================

async def log_request(request: InferenceRequest, response: InferenceResponse):
    """요청 로깅"""
    log_data = {
        "timestamp": datetime.now().isoformat(),
        "request_id": response.request_id,
        "agent_type": request.agent_type,
        "prompt_length": len(request.prompt),
        "response_length": len(response.response),
        "tokens_generated": response.tokens_generated,
        "response_time": response.response_time,
        "model_name": response.model_name
    }
    
    logger.info(f"Request processed: {log_data}")

# ============================================================================
# 메인 실행부
# ============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="vLLM High-Performance Inference Server")
    parser.add_argument("--host", default="0.0.0.0", help="Host address")
    parser.add_argument("--port", type=int, default=8000, help="Port number")
    parser.add_argument("--model", default="microsoft/DialoGPT-large", help="Model name")
    parser.add_argument("--tensor-parallel", type=int, default=0, help="Tensor parallel size (0=auto)")
    parser.add_argument("--max-model-len", type=int, default=4096, help="Max model length")
    parser.add_argument("--gpu-memory-util", type=float, default=0.9, help="GPU memory utilization")
    
    args = parser.parse_args()
    
    # 텐서 병렬 크기 자동 설정
    if args.tensor_parallel == 0:
        args.tensor_parallel = torch.cuda.device_count() if torch.cuda.is_available() else 1
    
    # 전역 매니저 설정 업데이트
    if 'vllm_manager' not in globals() or not vllm_manager:
        VLLMManager.__defaults__ = (
            args.model,
            args.tensor_parallel, 
            args.max_model_len,
            args.gpu_memory_util
        )
    
    uvicorn.run(
        "vllm_inference_server:app",
        host=args.host,
        port=args.port,
        workers=1,  # vLLM은 단일 워커에서 내부적으로 병렬 처리
        reload=False,
        log_level="info"
    )