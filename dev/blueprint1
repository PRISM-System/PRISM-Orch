# docker-compose.yml
# vLLM 고성능 추론 서버 Docker 구성

version: '3.8'

services:
  # vLLM 추론 서버
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: vllm_inference_server
    ports:
      - "8000:8000"
    environment:
      - MODEL_NAME=microsoft/DialoGPT-large
      - TENSOR_PARALLEL_SIZE=2
      - MAX_MODEL_LEN=4096
      - GPU_MEMORY_UTILIZATION=0.9
      - CUDA_VISIBLE_DEVICES=0,1  # 사용할 GPU 지정
    volumes:
      - model_cache:/root/.cache/huggingface
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2  # GPU 개수
              capabilities: [gpu]
    shm_size: 16gb  # 공유 메모리 크기 (중요!)

  # Redis (캐싱용)
  redis:
    image: redis:7-alpine
    container_name: vllm_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru

  # Nginx (로드 밸런서 + 캐싱)
  nginx:
    image: nginx:alpine
    container_name: vllm_nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - vllm-server
    restart: unless-stopped

  # 모니터링 (Prometheus + Grafana)
  prometheus:
    image: prom/prometheus:latest
    container_name: vllm_prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: vllm_grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    restart: unless-stopped

volumes:
  model_cache:
  redis_data:
  prometheus_data:
  grafana_data:

---
# Dockerfile.vllm
FROM nvidia/cuda:12.1-devel-ubuntu22.04

WORKDIR /app

# 시스템 패키지 설치
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Python 패키지 설치
COPY requirements-vllm.txt .
RUN pip3 install --no-cache-dir -r requirements-vllm.txt

# 애플리케이션 코드 복사
COPY vllm_inference_server.py .
COPY config/ ./config/

# 로그 디렉토리 생성
RUN mkdir -p /app/logs

# 모델 캐시 디렉토리
ENV HF_HOME=/root/.cache/huggingface
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface

# 포트 노출
EXPOSE 8000

# 헬스체크
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# 서버 실행
CMD ["python3", "vllm_inference_server.py", "--host", "0.0.0.0", "--port", "8000"]

---
# requirements-vllm.txt
vllm==0.3.2
fastapi==0.104.1
uvicorn[standard]==0.24.0
torch==2.1.0
transformers==4.36.0
accelerate==0.25.0
pydantic==2.5.0
psutil==5.9.6
gputil==1.4.0
numpy==1.24.3
aiohttp==3.9.1
redis==5.0.1

---
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream vllm_backend {
        server vllm-server:8000 max_fails=3 fail_timeout=30s;
        # 여러 인스턴스 추가 시
        # server vllm-server-2:8000 max_fails=3 fail_timeout=30s;
    }

    # 캐싱 설정
    proxy_cache_path /tmp/nginx_cache levels=1:2 keys_zone=llm_cache:10m max_size=1g inactive=60m;

    server {
        listen 80;
        server_name localhost;

        # 일반 요청
        location / {
            proxy_pass http://vllm_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            
            # 타임아웃 설정 (긴 추론 시간 고려)
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
            
            # 버퍼링 설정
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }

        # API 요청 (캐싱 적용)
        location /api/v1/generate {
            proxy_pass http://vllm_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            
            # 캐싱 (동일한 요청 결과 재사용)
            proxy_cache llm_cache;
            proxy_cache_key "$request_method$request_uri$request_body";
            proxy_cache_valid 200 10m;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
            
            # 캐시 헤더 추가
            add_header X-Cache-Status $upstream_cache_status;
            
            proxy_connect_timeout 300s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
        }

        # 헬스체크
        location /health {
            proxy_pass http://vllm_backend/health;
            proxy_connect_timeout 5s;
            proxy_send_timeout 5s;
            proxy_read_timeout 5s;
        }

        # 정적 파일 (모니터링 대시보드 등)
        location /static/ {
            alias /var/www/static/;
            expires 1d;
            add_header Cache-Control "public, immutable";
        }
    }
}

---
# config/models.yaml
# 지원 모델 설정

models:
  # 소형 모델 (개발/테스트용)
  small:
    name: "microsoft/DialoGPT-medium"
    tensor_parallel_size: 1
    max_model_len: 2048
    gpu_memory_utilization: 0.7
    description: "개발 및 테스트용 소형 모델"

  # 중형 모델 (일반 운영용)  
  medium:
    name: "microsoft/DialoGPT-large"
    tensor_parallel_size: 1
    max_model_len: 4096
    gpu_memory_utilization: 0.9
    description: "일반 운영용 중형 모델"

  # 대형 모델 (고성능 요구시)
  large:
    name: "meta-llama/Llama-2-13b-chat-hf"
    tensor_parallel_size: 2
    max_model_len: 4096
    gpu_memory_utilization: 0.95
    description: "고성능 대형 모델"

  # 초대형 모델 (최고 성능)
  xlarge:
    name: "meta-llama/Llama-2-70b-chat-hf"
    tensor_parallel_size: 4
    max_model_len: 4096
    gpu_memory_utilization: 0.95
    description: "최고 성능 초대형 모델"

# 에이전트별 권장 모델
agent_recommendations:
  orchestration: "medium"    # 복잡한 계획 수립
  monitoring: "small"        # 빠른 분석 응답
  prediction: "large"        # 정확한 예측 해석
  control: "medium"          # 안전한 제어 결정

---
# config/optimization.yaml
# 성능 최적화 설정

# vLLM 엔진 최적화
engine_optimization:
  # 메모리 최적화
  gpu_memory_utilization: 0.9
  swap_space: 4  # GB
  cpu_offload_gb: 0
  
  # 배치 처리 최적화
  max_num_seqs: 256
  max_num_batched_tokens: 8192
  max_paddings: 256
  
  # CUDA 최적화
  enforce_eager: false  # CUDA 그래프 사용
  use_v2_block_manager: true
  enable_prefix_caching: true
  
  # 토큰 생성 최적화
  disable_log_stats: true
  disable_log_requests: false

# 시스템 최적화
system_optimization:
  # 프로세스 설정
  worker_use_ray: false
  max_parallel_loading_workers: 4
  
  # 네트워크 최적화
  served_model_name: null
  chat_template: null
  response_role: "assistant"
  
  # 로깅 설정
  log_level: "INFO"
  log_requests: true
  log_stats_interval: 10

# 하드웨어별 권장 설정
hardware_profiles:
  # 단일 A100 80GB
  single_a100_80gb:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.95
    max_model_len: 8192
    max_num_seqs: 512
    
  # 듀얼 A100 80GB  
  dual_a100_80gb:
    tensor_parallel_size: 2
    gpu_memory_utilization: 0.95
    max_model_len: 8192
    max_num_seqs: 1024
    
  # 4x A100 80GB
  quad_a100_80gb:
    tensor_parallel_size: 4
    gpu_memory_utilization: 0.95
    max_model_len: 8192
    max_num_seqs: 2048
    
  # RTX 4090 (개발용)
  rtx_4090:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    max_model_len: 4096
    max_num_seqs: 128

---
# deploy.sh
#!/bin/bash
# vLLM 서버 배포 스크립트

set -e

echo "=== vLLM High-Performance Inference Server Deployment ==="

# 환경 변수 설정
export MODEL_NAME=${MODEL_NAME:-"microsoft/DialoGPT-large"}
export TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
export GPU_COUNT=${GPU_COUNT:-1}

# GPU 확인
echo "Checking GPU availability..."
nvidia-smi

# CUDA 버전 확인
echo "CUDA Version:"
nvcc --version

# Docker 이미지 빌드
echo "Building Docker image..."
docker build -f Dockerfile.vllm -t vllm-inference-server .

# 기존 컨테이너 정리
echo "Cleaning up existing containers..."
docker-compose down

# 새 서비스 시작
echo "Starting services..."
docker-compose up -d

# 서비스 상태 확인
echo "Checking service status..."
sleep 30
docker-compose ps

# 헬스체크
echo "Health check..."
for i in {1..10}; do
    if curl -f http://localhost:8000/health; then
        echo "✅ Server is healthy!"
        break
    else
        echo "⏳ Waiting for server to be ready... ($i/10)"
        sleep 10
    fi
done

# 성능 테스트
echo "Running performance test..."
curl -X POST http://localhost:8000/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "agent_type": "monitoring",
    "prompt": "센서 온도가 195도입니다. 정상 범위는 150-200도인데 상황을 분석해주세요.",
    "max_tokens": 256
  }'

echo ""
echo "✅ Deployment complete!"
echo "📊 Server URL: http://localhost:8000"
echo "🏥 Health Check: http://localhost:8000/health"
echo "📈 Stats: http://localhost:8000/api/v1/stats"
echo "🔍 Grafana: http://localhost:3000 (admin/admin)"

---
# benchmark.py
#!/usr/bin/env python3
"""
vLLM 서버 성능 벤치마크 스크립트
"""

import asyncio
import aiohttp
import time
import json
import statistics
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict

class VLLMBenchmark:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []
    
    async def single_request(self, session: aiohttp.ClientSession, request_data: dict) -> Dict:
        """단일 요청 실행"""
        start_time = time.time()
        
        try:
            async with session.post(
                f"{self.base_url}/api/v1/generate",
                json=request_data,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
                end_time = time.time()
                
                return {
                    "success": True,
                    "response_time": end_time - start_time,
                    "tokens_generated": result.get("tokens_generated", 0),
                    "status_code": response.status
                }
        except Exception as e:
            return {
                "success": False,
                "response_time": time.time() - start_time,
                "error": str(e),
                "tokens_generated": 0
            }
    
    async def concurrent_benchmark(self, concurrency: int, total_requests: int):
        """동시성 벤치마크"""
        print(f"\n🚀 Running benchmark: {total_requests} requests with {concurrency} concurrency")
        
        # 테스트 요청 데이터
        request_data = {
            "agent_type": "monitoring",
            "prompt": "제조 라인에서 온도 이상이 감지되었습니다. 현재 온도는 195도이고 정상 범위는 150-200도입니다. 상황을 분석하고 권장 조치를 제공해주세요.",
            "max_tokens": 256,
            "temperature": 0.1
        }
        
        start_time = time.time()
        
        async with aiohttp.ClientSession() as session:
            # 동시 요청 실행
            semaphore = asyncio.Semaphore(concurrency)
            
            async def limited_request():
                async with semaphore:
                    return await self.single_request(session, request_data)
            
            tasks = [limited_request() for _ in range(total_requests)]
            results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # 결과 분석
        successful_requests = [r for r in results if r["success"]]
        failed_requests = [r for r in results if not r["success"]]
        
        if successful_requests:
            response_times = [r["response_time"] for r in successful_requests]
            tokens_generated = [r["tokens_generated"] for r in successful_requests]
            total_tokens = sum(tokens_generated)
            
            print(f"📊 Results:")
            print(f"  ✅ Successful requests: {len(successful_requests)}/{total_requests}")
            print(f"  ❌ Failed requests: {len(failed_requests)}")
            print(f"  🕐 Total time: {total_time:.2f}s")
            print(f"  📈 Requests/sec: {len(successful_requests)/total_time:.2f}")
            print(f"  🎯 Tokens/sec: {total_tokens/total_time:.2f}")
            print(f"  ⏱️  Response time - Avg: {statistics.mean(response_times):.2f}s")
            print(f"  ⏱️  Response time - P50: {statistics.median(response_times):.2f}s")
            print(f"  ⏱️  Response time - P95: {sorted(response_times)[int(len(response_times)*0.95)]:.2f}s")
            print(f"  ⏱️  Response time - P99: {sorted(response_times)[int(len(response_times)*0.99)]:.2f}s")
            
            # 결과 저장
            self.results.append({
                "concurrency": concurrency,
                "total_requests": total_requests,
                "successful_requests": len(successful_requests),
                "requests_per_sec": len(successful_requests)/total_time,
                "tokens_per_sec": total_tokens/total_time,
                "avg_response_time": statistics.mean(response_times),
                "p95_response_time": sorted(response_times)[int(len(response_times)*0.95)],
                "total_time": total_time
            })
    
    async def run_full_benchmark(self):
        """전체 벤치마크 실행"""
        print("🎯 vLLM Performance Benchmark Starting...")
        
        # 다양한 동시성 레벨 테스트
        test_scenarios = [
            (1, 10),    # 1 concurrent, 10 requests
            (5, 50),    # 5 concurrent, 50 requests  
            (10, 100),  # 10 concurrent, 100 requests
            (20, 200),  # 20 concurrent, 200 requests
            (50, 500),  # 50 concurrent, 500 requests
        ]
        
        for concurrency, requests in test_scenarios:
            await self.concurrent_benchmark(concurrency, requests)
            await asyncio.sleep(2)  # 서버 휴식 시간
        
        # 결과 요약
        print("\n📈 Benchmark Summary:")
        print("Concurrency | Requests | RPS   | Tokens/s | Avg RT | P95 RT")
        print("-" * 60)
        for result in self.results:
            print(f"{result['concurrency']:10d} | {result['total_requests']:8d} | "
                  f"{result['requests_per_sec']:5.1f} | {result['tokens_per_sec']:8.1f} | "
                  f"{result['avg_response_time']:6.2f} | {result['p95_response_time']:6.2f}")
        
        # JSON 결과 저장
        with open("benchmark_results.json", "w") as f:
            json.dump(self.results, f, indent=2)
        
        print(f"\n💾 Results saved to benchmark_results.json")

if __name__ == "__main__":
    benchmark = VLLMBenchmark()
    asyncio.run(benchmark.run_full_benchmark())

---
# monitor.py
"""
vLLM 서버 모니터링 스크립트
"""

import asyncio
import aiohttp
import time
import json
from datetime import datetime

async def monitor_server(base_url: str = "http://localhost:8000"):
    """서버 모니터링"""
    print("🔍 vLLM Server Monitoring Started...")
    
    async with aiohttp.ClientSession() as session:
        while True:
            try:
                # 헬스체크
                async with session.get(f"{base_url}/health") as response:
                    health_data = await response.json()
                
                # 통계 조회
                async with session.get(f"{base_url}/api/v1/stats") as response:
                    stats_data = await response.json()
                
                # 현재 시간
                now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                # 모니터링 정보 출력
                print(f"\n[{now}] Server Status:")
                print(f"  🟢 Status: {health_data['status']}")
                print(f"  💾 RAM Usage: {health_data['memory_usage']['ram_percent']:.1f}%")
                print(f"  🔥 GPU Count: {health_data['gpu_count']}")
                print(f"  📊 Active Requests: {health_data['active_requests']}")
                print(f"  📈 Total Requests: {health_data['total_requests']}")
                print(f"  ⚡ Throughput: {health_data['throughput']:.2f} tokens/sec")
                print(f"  ⏰ Uptime: {health_data['uptime']:.0f}s")
                
                if 'performance' in stats_data:
                    perf = stats_data['performance']
                    print(f"  🎯 Avg Tokens/Request: {perf['average_tokens_per_request']:.1f}")
                    print(f"  📊 Total Tokens: {perf['total_tokens_generated']}")
                
            except Exception as e:
                print(f"❌ Monitoring error: {e}")
            
            await asyncio.sleep(10)  # 10초마다 모니터링

if __name__ == "__main__":
    asyncio.run(monitor_server())